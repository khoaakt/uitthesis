FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535520000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 4 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 3, 5.00000000000000000000e-001) (22, 3, 5.00000000000000000000e-001) (22, 3, 5.00000000000000000000e-001) (0, 3, 0.00000000000000000000e+000) (4, 3, 5.00000000000000000000e-001) (4, 3, 5.00000000000000000000e-001) (4, 3, 5.00000000000000000000e-001) (0, 3, 0.00000000000000000000e+000) (4, 3, 5.00000000000000000000e-001) (0, 3, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.96369857788085940000e+001) (1, -2.07393383979797360000e+000) (2, -7.17574596405029300000e+000) (3, -2.80175757408142090000e+000) (4, 1.11428184509277340000e+001) (5, 1.69510431587696080000e-002) (6, -8.33262729644775390000e+000) (7, 6.77257850766181950000e-002) (8, 1.44872198104858400000e+001) (9, -2.89189958572387700000e+000) (10, -1.95196723937988280000e+000) (11, 4.72733139991760250000e-001) (12, -7.62931108474731450000e+000) (13, -1.32287073135375980000e+000) (14, 5.12568855285644530000e+000) (15, -2.32876479625701900000e-001) (16, -2.02160644531250000000e+000) (17, 4.87550544738769530000e+000) (18, -2.04931545257568360000e+000) (19, 5.65005207061767580000e+000) (20, -2.48145532608032230000e+000) (21, -2.09759664535522460000e+000) (0, -3.35914134979248050000e+000) (1, -8.17633914947509770000e+000) (2, -9.33229732513427730000e+000) (3, 1.06704120635986330000e+001) (4, -1.43852001953125000000e+003) (5, -5.99572181701660160000e+000) (6, 2.44718875885009770000e+001) (7, -6.02714824676513670000e+000) (8, 3.50684165954589840000e+001) (9, 2.31503562927246090000e+001) (10, -3.91697931289672850000e+000) (11, 1.37482404708862300000e+000) (12, -1.92253351211547850000e+000) (13, 2.76121091842651370000e+000) (14, -2.50292930603027340000e+001) (15, -3.72643585205078120000e+001) (16, -1.30727443695068360000e+001) (17, 4.31091957092285160000e+001) (18, 2.79028072357177730000e+001) (19, -1.43852001953125000000e+003) (20, 1.50691566467285160000e+001) (21, -5.57197999954223630000e+000) (0, -2.26279234886169430000e+000) (1, -2.45788764953613280000e+000) (2, -2.05560851097106930000e+000) (3, -2.20297956466674800000e+000) (4, -2.05471954345703130000e+001) (5, -1.95162665843963620000e+000) (6, 1.46003761291503910000e+001) (7, -2.04164123535156250000e+000) (8, 1.52715997695922850000e+001) (9, -2.26779150962829590000e+000) (10, -2.07358694076538090000e+000) (11, -2.43475675582885740000e+000) (12, -2.58308172225952150000e+000) (13, 4.94691705703735350000e+000) (14, -1.60961360931396480000e+001) (15, -2.08112645149230960000e+000) (16, -2.79103231430053710000e+000) (17, 2.03585553169250490000e+000) (18, -1.26383423805236820000e+000) (19, -4.88445520401000980000e+000) (20, -1.58862626552581790000e+000) (21, -2.07177329063415530000e+000) (22, -9.88566398620605470000e-001) (23, 6.32466793060302730000e-001) (24, -1.01760280132293700000e+000) (25, -3.49658094346523280000e-002) (22, -1.03573906421661380000e+000) (23, 6.66114449501037600000e-001) (24, -1.09945714473724370000e+000) (25, -8.21714103221893310000e-002) (22, -1.07383120059967040000e+000) (23, 7.24288165569305420000e-001) (24, -1.07219254970550540000e+000) (25, -8.73227789998054500000e-002) (26, 3.07655990123748780000e-001) (27, 3.69969457387924190000e-001) (28, 3.12594264745712280000e-001) (29, 3.09607535600662230000e-001) 
